#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AQUASKY AIQA Monitor - æ¸¬è©¦å‰©é¤˜æ¨¡å‹è…³æœ¬
å°ˆé–€æ¸¬è©¦ Perplexity Sonarã€Grok Betaã€Grok 2 é€™ä¸‰å€‹æ¨¡å‹
"""

import os
import sys
import json
import requests
import configparser
import time
from datetime import datetime
from pathlib import Path
import pandas as pd

class RemainingModelsTest:
    """æ¸¬è©¦å‰©é¤˜æ¨¡å‹çš„è™•ç†å™¨"""
    
    def __init__(self):
        print(f"ğŸ“ åˆå§‹åŒ–æ¸¬è©¦å™¨ - å·¥ä½œç›®éŒ„: {Path.cwd()}")
        self.project_root = Path.cwd()
        self.output_dir = self.project_root / "outputs"
        self.output_dir.mkdir(exist_ok=True)
        print(f"ğŸ“‚ è¼¸å‡ºç›®éŒ„: {self.output_dir}")
        
        # éœ€è¦æ¸¬è©¦çš„ä¸‰å€‹æ¨¡å‹
        self.test_models = [
            {
                "id": "perplexity/llama-3.1-sonar-small-128k-online",
                "name": "Perplexity Sonar",
                "status": "required"   # å¿…éœ€æ¨¡å‹
            },
            {
                "id": "x-ai/grok-beta",
                "name": "Grok Beta",
                "status": "testing"   # å¾…æ¸¬è©¦
            },
            {
                "id": "x-ai/grok-2",
                "name": "Grok 2",
                "status": "testing"   # å‚™ç”¨ Grok æ¨¡å‹
            }
        ]
        
        self.api_key = None
        self.questions = []
        
    def load_config(self):
        """è¼‰å…¥é…ç½®æª”æ¡ˆ"""
        print("\nğŸ” é–‹å§‹è¼‰å…¥é…ç½®æª”æ¡ˆ...")
        config = configparser.ConfigParser()
        config_path = Path("config.ini")
        
        if not config_path.exists():
            print("âŒ æ‰¾ä¸åˆ° config.ini æª”æ¡ˆ")
            return False
        
        config.read(config_path, encoding='utf-8')
        self.api_key = config.get('api_keys', 'OPENROUTER_API_KEY', fallback=None)
        
        if not self.api_key or self.api_key == 'your_openrouter_api_key_here':
            print("âŒ è«‹åœ¨ config.ini ä¸­è¨­å®šæœ‰æ•ˆçš„ OPENROUTER_API_KEY")
            return False
        
        print("âœ… é…ç½®æª”æ¡ˆè¼‰å…¥æˆåŠŸ")
        return True
    
    def extract_questions(self):
        """å¾å•é¡Œæª”æ¡ˆä¸­æå–æ‰€æœ‰ 20 å€‹å•é¡Œ"""
        print("\nğŸ” é–‹å§‹æå–å•é¡Œ...")
        questions_file = Path("AQUASKY AEO ç›£æ§å°ˆæ¡ˆ - é»ƒé‡‘å•é¡Œåº« V3.0.md")
        
        if not questions_file.exists():
            print(f"âŒ æ‰¾ä¸åˆ°å•é¡Œæª”æ¡ˆ: {questions_file}")
            return False
        
        try:
            with open(questions_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            questions = []
            lines = content.split('\n')
            
            for line in lines:
                line = line.strip()
                if line and any(line.startswith(f"{i}.") for i in range(1, 21)):
                    question = line.split('.', 1)[1].strip()
                    if '(' in question and ')' in question:
                        question = question.split('(')[0].strip()
                    questions.append(question)
            
            self.questions = questions
            print(f"âœ… æˆåŠŸæå– {len(questions)} å€‹å•é¡Œ")
            return len(questions) == 20
            
        except Exception as e:
            print(f"âŒ è®€å–å•é¡Œæª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return False
    
    def test_model_availability(self, model_id):
        """æ¸¬è©¦æ¨¡å‹æ˜¯å¦å¯ç”¨"""
        print(f"\nğŸ§ª æ¸¬è©¦æ¨¡å‹ {model_id} æ˜¯å¦å¯ç”¨...")
        url = "https://openrouter.ai/api/v1/chat/completions"
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://github.com/aquasky-aiqa-monitor",
            "X-Title": "AQUASKY AIQA Monitor"
        }
        
        # ä½¿ç”¨ç°¡å–®çš„æ¸¬è©¦å•é¡Œ
        data = {
            "model": model_id,
            "messages": [
                {
                    "role": "user",
                    "content": "Hello, please respond with 'OK' if you can understand this message."
                }
            ],
            "max_tokens": 10,
            "temperature": 0.1
        }
        
        try:
            print(f"  ğŸ”„ ç™¼é€æ¸¬è©¦è«‹æ±‚...")
            response = requests.post(url, headers=headers, json=data, timeout=30)
            
            print(f"  ğŸ“Š HTTP ç‹€æ…‹ç¢¼: {response.status_code}")
            
            if response.status_code == 200:
                result = response.json()
                if 'choices' in result and len(result['choices']) > 0:
                    answer = result['choices'][0]['message']['content']
                    print(f"  âœ… æ¨¡å‹å¯ç”¨ï¼Œå›æ‡‰: {answer}")
                    return True
                else:
                    print(f"  âŒ æ¨¡å‹å›æ‡‰æ ¼å¼ç•°å¸¸: {result}")
                    return False
            else:
                print(f"  âŒ æ¨¡å‹ä¸å¯ç”¨ (HTTP {response.status_code})")
                try:
                    error_info = response.json()
                    print(f"      éŒ¯èª¤è©³æƒ…: {json.dumps(error_info, indent=2, ensure_ascii=False)}")
                except:
                    print(f"      éŒ¯èª¤å…§å®¹: {response.text}")
                return False
                
        except Exception as e:
            print(f"  âŒ æ¸¬è©¦æ¨¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return False
    
    def call_llm_api(self, model_id, question, question_num):
        """å‘¼å« LLM API"""
        url = "https://openrouter.ai/api/v1/chat/completions"
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://github.com/aquasky-aiqa-monitor",
            "X-Title": "AQUASKY AIQA Monitor"
        }
        
        data = {
            "model": model_id,
            "messages": [
                {
                    "role": "user",
                    "content": question
                }
            ],
            "max_tokens": 1000,
            "temperature": 0.7
        }
        
        try:
            print(f"  ğŸ”„ è™•ç†å•é¡Œ {question_num}/20...")
            response = requests.post(url, headers=headers, json=data, timeout=60)
            
            if response.status_code == 200:
                result = response.json()
                if 'choices' in result and len(result['choices']) > 0:
                    answer = result['choices'][0]['message']['content']
                    usage = result.get('usage', {})
                    
                    print(f"  âœ… å•é¡Œ {question_num} å®Œæˆ (Token: {usage.get('total_tokens', 0)})")
                    return {
                        'success': True,
                        'answer': answer,
                        'usage': usage
                    }
                else:
                    print(f"  âŒ å•é¡Œ {question_num} - API å›æ‡‰æ ¼å¼ç•°å¸¸")
                    return {'success': False, 'error': 'Invalid response format'}
            else:
                print(f"  âŒ å•é¡Œ {question_num} - API éŒ¯èª¤: {response.status_code}")
                try:
                    error_info = response.json()
                    print(f"      éŒ¯èª¤è©³æƒ…: {json.dumps(error_info, indent=2, ensure_ascii=False)}")
                    return {'success': False, 'error': f'HTTP {response.status_code}: {error_info}'}
                except:
                    return {'success': False, 'error': f'HTTP {response.status_code}: {response.text}'}
                    
        except Exception as e:
            print(f"  âŒ å•é¡Œ {question_num} - ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def process_single_model(self, model_info):
        """è™•ç†å–®ä¸€æ¨¡å‹çš„æ‰€æœ‰å•é¡Œ"""
        model_id = model_info["id"]
        model_name = model_info["name"]
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        print(f"\n{'='*80}")
        print(f"ğŸ¤– é–‹å§‹è™•ç†æ¨¡å‹: {model_name}")
        print(f"ğŸ“ æ¨¡å‹ID: {model_id}")
        print(f"ğŸ”– ç‹€æ…‹: {model_info['status']}")
        print(f"{'='*80}")
        
        # å…ˆæ¸¬è©¦æ¨¡å‹å¯ç”¨æ€§
        if not self.test_model_availability(model_id):
            print(f"âŒ æ¨¡å‹ {model_name} ä¸å¯ç”¨ï¼Œè·³éè™•ç†")
            return False, 0, 0
        
        print(f"\nâœ… æ¨¡å‹ {model_name} å¯ç”¨ï¼Œé–‹å§‹è™•ç† 20 å€‹å•é¡Œ...")
        
        results = []
        successful_count = 0
        total_tokens = 0
        
        for i, question in enumerate(self.questions, 1):
            result = self.call_llm_api(model_id, question, i)
            results.append(result)
            
            if result['success']:
                successful_count += 1
                total_tokens += result.get('usage', {}).get('total_tokens', 0)
            
            # æ¯å€‹å•é¡Œä¹‹é–“æš«åœ 3 ç§’
            if i < len(self.questions):
                time.sleep(3)
        
        # å„²å­˜çµæœ
        self.save_model_results(model_id, model_name, results, timestamp)
        
        print(f"\nğŸ“Š {model_name} è™•ç†å®Œæˆ:")
        print(f"  âœ… æˆåŠŸ: {successful_count}/20 å€‹å•é¡Œ")
        print(f"  ğŸ“ˆ ç¸½Token: {total_tokens}")
        print(f"{'='*80}")
        
        return True, successful_count, total_tokens
    
    def save_model_results(self, model_id, model_name, results, timestamp):
        """å„²å­˜å–®ä¸€æ¨¡å‹çš„çµæœ"""
        # æº–å‚™è³‡æ–™
        data = []
        
        for i, (question, result) in enumerate(zip(self.questions, results), 1):
            row = {
                'å•é¡Œç·¨è™Ÿ': i,
                'å•é¡Œ': question,
                'å›ç­”': result.get('answer', 'è™•ç†å¤±æ•—') if result['success'] else 'è™•ç†å¤±æ•—',
                'ç‹€æ…‹': 'æˆåŠŸ' if result['success'] else 'å¤±æ•—',
                'éŒ¯èª¤è¨Šæ¯': result.get('error', '') if not result['success'] else '',
                'è¼¸å…¥Token': result.get('usage', {}).get('prompt_tokens', 0) if result['success'] else 0,
                'è¼¸å‡ºToken': result.get('usage', {}).get('completion_tokens', 0) if result['success'] else 0,
                'ç¸½Token': result.get('usage', {}).get('total_tokens', 0) if result['success'] else 0
            }
            data.append(row)
        
        # ç”Ÿæˆå®‰å…¨çš„æª”æ¡ˆåç¨±
        safe_model_name = model_id.replace('/', '_').replace('\\', '_')
        
        # å„²å­˜ Excel
        df = pd.DataFrame(data)
        excel_filename = f"AQUASKY_AIQA_{safe_model_name}_{timestamp}.xlsx"
        excel_path = self.output_dir / excel_filename
        df.to_excel(excel_path, index=False, engine='openpyxl')
        
        # å„²å­˜ Markdown
        md_filename = f"AQUASKY_AIQA_{safe_model_name}_{timestamp}.md"
        md_path = self.output_dir / md_filename
        
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(f"# AQUASKY AIQA Monitor - {model_name} æ¸¬è©¦å ±å‘Š\n\n")
            f.write(f"**ç”Ÿæˆæ™‚é–“**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**æ¨¡å‹**: {model_id}\n")
            f.write(f"**æ¨¡å‹åç¨±**: {model_name}\n\n")
            
            # çµ±è¨ˆè³‡è¨Š
            successful_count = sum(1 for r in results if r['success'])
            total_input_tokens = sum(r.get('usage', {}).get('prompt_tokens', 0) for r in results if r['success'])
            total_output_tokens = sum(r.get('usage', {}).get('completion_tokens', 0) for r in results if r['success'])
            
            f.write(f"## çµ±è¨ˆè³‡è¨Š\n\n")
            f.write(f"- **æˆåŠŸå•é¡Œæ•¸**: {successful_count}/20\n")
            f.write(f"- **ç¸½è¼¸å…¥Token**: {total_input_tokens}\n")
            f.write(f"- **ç¸½è¼¸å‡ºToken**: {total_output_tokens}\n")
            f.write(f"- **ç¸½Token**: {total_input_tokens + total_output_tokens}\n\n")
            
            # å•é¡Œå’Œå›ç­”
            for i, (question, result) in enumerate(zip(self.questions, results), 1):
                f.write(f"## å•é¡Œ {i}\n\n")
                f.write(f"**å•é¡Œ**: {question}\n\n")
                if result['success']:
                    f.write(f"**å›ç­”**: {result['answer']}\n\n")
                    usage = result.get('usage', {})
                    f.write(f"**Tokenä½¿ç”¨**: è¼¸å…¥ {usage.get('prompt_tokens', 0)}, è¼¸å‡º {usage.get('completion_tokens', 0)}, ç¸½è¨ˆ {usage.get('total_tokens', 0)}\n\n")
                else:
                    f.write(f"**ç‹€æ…‹**: è™•ç†å¤±æ•—\n")
                    f.write(f"**éŒ¯èª¤**: {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}\n\n")
                f.write("---\n\n")
        
        print(f"  ğŸ“ çµæœå·²å„²å­˜:")
        print(f"    Excel: {excel_filename}")
        print(f"    Markdown: {md_filename}")
    
    def run_test(self):
        """åŸ·è¡Œæ¸¬è©¦"""
        print("ğŸš€ AQUASKY AIQA Monitor - å‰©é¤˜æ¨¡å‹æ¸¬è©¦ç³»çµ±")
        print("=" * 80)
        print(f"ğŸ•°ï¸ é–‹å§‹æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # è¼‰å…¥é…ç½®
        print("ğŸ”„ æ­¥é©Ÿ 1: è¼‰å…¥é…ç½®æª”æ¡ˆ")
        if not self.load_config():
            print("âŒ è¼‰å…¥é…ç½®å¤±æ•—")
            return False
        
        # æå–å•é¡Œ
        print("ğŸ”„ æ­¥é©Ÿ 2: æå–å•é¡Œ")
        if not self.extract_questions():
            print("âŒ æå–å•é¡Œå¤±æ•—")
            return False
        
        print(f"\nğŸ“‹ å°‡æ¸¬è©¦ä»¥ä¸‹ {len(self.test_models)} å€‹æ¨¡å‹:")
        for i, model in enumerate(self.test_models, 1):
            status_icon = "ğŸ”´" if model["status"] == "required" else "ğŸ§ª"
            print(f"  {i}. {status_icon} {model['name']} ({model['id']})")
        
        print(f"\nğŸ’¡ æ¯å€‹æ¨¡å‹å°‡è™•ç† 20 å€‹å•é¡Œï¼Œå®Œæˆå¾Œè‡ªå‹•å„²å­˜çµæœ")
        print("=" * 80)
        
        # é–‹å§‹è™•ç†æ¯å€‹æ¨¡å‹
        total_successful = 0
        total_tokens = 0
        processed_models = 0
        
        for i, model_info in enumerate(self.test_models, 1):
            print(f"\nğŸ”„ é€²åº¦: {i}/{len(self.test_models)}")
            
            try:
                success, successful, tokens = self.process_single_model(model_info)
                
                if success:
                    processed_models += 1
                    total_successful += successful
                    total_tokens += tokens
                
                # æ¨¡å‹ä¹‹é–“æš«åœ 10 ç§’ï¼ˆå¢åŠ æš«åœæ™‚é–“é¿å…é »ç‡é™åˆ¶ï¼‰
                if i < len(self.test_models):
                    print(f"â¸ï¸ æš«åœ 10 ç§’å¾Œè™•ç†ä¸‹ä¸€å€‹æ¨¡å‹...")
                    time.sleep(10)
                    
            except KeyboardInterrupt:
                print(f"\nâ¸ï¸ ä½¿ç”¨è€…ä¸­æ–·è™•ç†ï¼Œå·²å®Œæˆ {processed_models} å€‹æ¨¡å‹")
                break
            except Exception as e:
                print(f"\nâŒ è™•ç†æ¨¡å‹ {model_info['name']} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                import traceback
                traceback.print_exc()
                continue
        
        # é¡¯ç¤ºç¸½çµ
        print(f"\nğŸ‰ æ¸¬è©¦å®Œæˆï¼")
        print(f"ğŸ“Š æˆåŠŸè™•ç†æ¨¡å‹æ•¸: {processed_models}")
        print(f"ğŸ“ˆ ç¸½æˆåŠŸå•é¡Œæ•¸: {total_successful}")
        print(f"ğŸ’° ç¸½Tokenä½¿ç”¨: {total_tokens}")
        print(f"ğŸ“ æ‰€æœ‰çµæœæª”æ¡ˆå·²å„²å­˜è‡³ outputs/ ç›®éŒ„")
        print("=" * 80)
        
        return True

def main():
    """ä¸»ç¨‹å¼"""
    print("===== é–‹å§‹åŸ·è¡Œå‰©é¤˜æ¨¡å‹æ¸¬è©¦ =====")
    print(f"ğŸ•°ï¸ åŸ·è¡Œæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“ å·¥ä½œç›®éŒ„: {Path.cwd()}")
    
    try:
        print("ğŸš€ åˆå§‹åŒ–æ¸¬è©¦å™¨...")
        tester = RemainingModelsTest()
        print("âœ… æ¸¬è©¦å™¨åˆå§‹åŒ–æˆåŠŸ")
        
        print("ğŸ“ é–‹å§‹åŸ·è¡Œæ¸¬è©¦...")
        result = tester.run_test()
        print(f"ğŸ æ¸¬è©¦çµæœ: {'Success' if result else 'Failed'}")
        
    except Exception as e:
        import traceback
        print(f"\nâŒâŒâŒ ç™¼ç”Ÿæœªæ•ç²çš„éŒ¯èª¤: {str(e)}")
        print("è©³ç´°éŒ¯èª¤è¿½è¹¤:")
        traceback.print_exc()
    
    print("===== æ¸¬è©¦åŸ·è¡ŒçµæŸ =====")
    print(f"ğŸ•°ï¸ çµæŸæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

if __name__ == "__main__":
    main()
