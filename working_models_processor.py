#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AQUASKY AIQA Monitor - å·²é©—è­‰å¯ç”¨æ¨¡å‹è™•ç†å™¨
å°ˆæ³¨æ–¼å·²çŸ¥å¯ç”¨çš„æ¨¡å‹ï¼Œé¿å…å¡åœ¨ä¸å¯ç”¨çš„æ¨¡å‹ä¸Š
"""

import os
import sys
import json
import requests
import configparser
import time
from datetime import datetime
from pathlib import Path
import pandas as pd

class WorkingModelsProcessor:
    """å·²é©—è­‰å¯ç”¨æ¨¡å‹çš„è™•ç†å™¨"""
    
    def __init__(self):
        self.project_root = Path.cwd()
        self.output_dir = self.project_root / "outputs"
        self.output_dir.mkdir(exist_ok=True)
        
        # å·²é©—è­‰å¯ç”¨çš„æ¨¡å‹ï¼ˆæ ¹æ“šç”¨æˆ¶éœ€æ±‚èª¿æ•´ï¼‰
        self.working_models = [
            # å·²æˆåŠŸæ¸¬è©¦éçš„æ¨¡å‹
            {
                "id": "deepseek/deepseek-chat",
                "name": "DeepSeek Chat",
                "status": "verified"  # å·²é©—è­‰å¯ç”¨
            },
            {
                "id": "openai/gpt-4o-mini",
                "name": "GPT-4o Mini",
                "status": "verified"   # å·²é©—è­‰å¯ç”¨
            },
            # ä½¿ç”¨è€…è¦æ±‚ç§»é™¤ Claude 3 Haiku
            {
                "id": "google/gemini-flash-1.5",
                "name": "Gemini Flash 1.5",
                "status": "verified"   # å·²é©—è­‰å¯ç”¨
            },
            # å…¶å®ƒå·²é©—è­‰å¯ç”¨çš„æ¨¡å‹
            {
                "id": "anthropic/claude-3.5-sonnet",
                "name": "Claude 3.5 Sonnet",
                "status": "verified"   # å·²é©—è­‰å¯ç”¨
            },
            {
                "id": "meta-llama/llama-3.1-8b-instruct",
                "name": "Llama 3.1 8B",
                "status": "verified"   # å·²é©—è­‰å¯ç”¨
            },
            {
                "id": "mistralai/mistral-7b-instruct",
                "name": "Mistral 7B",
                "status": "verified"   # å·²é©—è­‰å¯ç”¨
            },
            # ä½¿ç”¨è€…æŒ‡å®šå¿…éœ€çš„ Perplexity æ¨¡å‹ï¼ˆå·²é©—è­‰å¯ç”¨ï¼‰
            {
                "id": "perplexity/sonar-pro",
                "name": "Perplexity Sonar Pro",
                "status": "verified",   # å·²é©—è­‰å¯ç”¨
                "api_type": "perplexity"  # ä½¿ç”¨ Perplexity ç›´æ¥ API
            },
            # OpenRouter å¯ç”¨çš„ Grok æ¨¡å‹ï¼ˆå·²æ¸¬è©¦ 2025-07-26ï¼‰
            {
                "id": "x-ai/grok-3-mini-beta",
                "name": "Grok 3 Mini Beta",
                "status": "verified"   # âœ… å·²é©—è­‰å¯ç”¨ï¼Œ2.74ç§’å›æ‡‰ï¼ˆæœ€å¿«ï¼‰
            },
            # ä¸å¯ç”¨çš„æ¨¡å‹ï¼ˆä¿ç•™ä½œç‚ºè¨˜éŒ„ï¼‰
            # {
            #     "id": "x-ai/grok-beta",
            #     "name": "Grok Beta",
            #     "status": "unavailable"   # âŒ HTTP 404 - No endpoints found
            # }
        ]
        
        self.openrouter_api_key = None
        self.perplexity_api_key = None
        self.questions = []
        
    def load_config(self):
        """è¼‰å…¥é…ç½®æª”æ¡ˆ"""
        print("\nğŸ” é–‹å§‹è¼‰å…¥é…ç½®æª”æ¡ˆ...")
        config = configparser.ConfigParser()
        config_path = Path("config.ini")
        
        if not config_path.exists():
            print("âŒ æ‰¾ä¸åˆ° config.ini æª”æ¡ˆ")
            return False
        
        config.read(config_path, encoding='utf-8')
        
        # è¼‰å…¥ OpenRouter API Key
        self.openrouter_api_key = config.get('api_keys', 'openrouter_api_key', fallback=None)
        
        # è¼‰å…¥ Perplexity API Key
        self.perplexity_api_key = config.get('api_keys', 'PERPLEXITY_API_KEY', fallback=None)
        
        if not self.openrouter_api_key or self.openrouter_api_key == 'your_openrouter_api_key_here':
            print("âŒ è«‹åœ¨ config.ini ä¸­è¨­å®šæœ‰æ•ˆçš„ openrouter_api_key")
            return False
        
        if not self.perplexity_api_key or self.perplexity_api_key == 'your_perplexity_api_key_here':
            print("âŒ è«‹åœ¨ config.ini ä¸­è¨­å®šæœ‰æ•ˆçš„ PERPLEXITY_API_KEY")
            return False
        
        return True
    
    def extract_questions(self):
        """å¾å•é¡Œæª”æ¡ˆä¸­æå–æ‰€æœ‰ 20 å€‹å•é¡Œ"""
        print("\nğŸ” é–‹å§‹æå–å•é¡Œ...")
        questions_file = Path("AQUASKY AEO ç›£æ§å°ˆæ¡ˆ - é»ƒé‡‘å•é¡Œåº« V2.0.md")
        
        if not questions_file.exists():
            print(f"âŒ æ‰¾ä¸åˆ°å•é¡Œæª”æ¡ˆ: {questions_file}")
            return False
        
        try:
            with open(questions_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            questions = []
            lines = content.split('\n')
            
            for line in lines:
                line = line.strip()
                if line and any(line.startswith(f"{i}.") for i in range(1, 21)):
                    question = line.split('.', 1)[1].strip()
                    if '(' in question and ')' in question:
                        question = question.split('(')[0].strip()
                    questions.append(question)
            
            self.questions = questions
            print(f"âœ… æˆåŠŸæå– {len(questions)} å€‹å•é¡Œ")
            return len(questions) == 20
            
        except Exception as e:
            print(f"âŒ è®€å–å•é¡Œæª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return False
    
    def test_model_availability(self, model_info):
        """æ¸¬è©¦æ¨¡å‹æ˜¯å¦å¯ç”¨ï¼ˆæ”¯æ´ OpenRouter å’Œ Perplexityï¼‰"""
        model_id = model_info["id"]
        api_type = model_info.get("api_type", "openrouter")
        
        print(f"\nğŸ§ª æ¸¬è©¦æ¨¡å‹ {model_id} æ˜¯å¦å¯ç”¨...")
        
        if api_type == "perplexity":
            # ä½¿ç”¨ Perplexity ç›´æ¥ API
            url = "https://api.perplexity.ai/chat/completions"
            headers = {
                "Authorization": f"Bearer {self.perplexity_api_key}",
                "Content-Type": "application/json"
            }
            actual_model_id = model_id.replace("perplexity/", "")
        else:
            # ä½¿ç”¨ OpenRouter API
            url = "https://openrouter.ai/api/v1/chat/completions"
            headers = {
                "Authorization": f"Bearer {self.openrouter_api_key}",
                "Content-Type": "application/json",
                "HTTP-Referer": "https://github.com/aquasky-aiqa-monitor",
                "X-Title": "AQUASKY AIQA Monitor"
            }
            actual_model_id = model_id
        
        # ä½¿ç”¨ç°¡å–®çš„æ¸¬è©¦å•é¡Œ
        data = {
            "model": actual_model_id,
            "messages": [
                {
                    "role": "user",
                    "content": "Hello, please respond with 'OK' if you can understand this message."
                }
            ],
            "max_tokens": 10,
            "temperature": 0.1
        }
        
        try:
            print(f"  ğŸ§ª æ¸¬è©¦æ¨¡å‹å¯ç”¨æ€§...")
            response = requests.post(url, headers=headers, json=data, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                if 'choices' in result and len(result['choices']) > 0:
                    print(f"  âœ… æ¨¡å‹å¯ç”¨")
                    return True
                else:
                    print(f"  âŒ æ¨¡å‹å›æ‡‰æ ¼å¼ç•°å¸¸")
                    return False
            else:
                print(f"  âŒ æ¨¡å‹ä¸å¯ç”¨ (HTTP {response.status_code})")
                try:
                    error_info = response.json()
                    print(f"      éŒ¯èª¤è©³æƒ…: {error_info}")
                except:
                    print(f"      éŒ¯èª¤å…§å®¹: {response.text}")
                return False
                
        except Exception as e:
            print(f"  âŒ æ¸¬è©¦æ¨¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return False
    
    def call_llm_api(self, model_info, question, question_num):
        """å‘¼å« LLM APIï¼ˆæ”¯æ´ OpenRouter å’Œ Perplexityï¼‰"""
        model_id = model_info["id"]
        api_type = model_info.get("api_type", "openrouter")
        
        if api_type == "perplexity":
            # ä½¿ç”¨ Perplexity ç›´æ¥ API
            url = "https://api.perplexity.ai/chat/completions"
            headers = {
                "Authorization": f"Bearer {self.perplexity_api_key}",
                "Content-Type": "application/json"
            }
            # Perplexity æ¨¡å‹ ID éœ€è¦ç§»é™¤ perplexity/ å‰ç¶´
            actual_model_id = model_id.replace("perplexity/", "")
        else:
            # ä½¿ç”¨ OpenRouter API
            url = "https://openrouter.ai/api/v1/chat/completions"
            headers = {
                "Authorization": f"Bearer {self.openrouter_api_key}",
                "Content-Type": "application/json",
                "HTTP-Referer": "https://github.com/aquasky-aiqa-monitor",
                "X-Title": "AQUASKY AIQA Monitor"
            }
            actual_model_id = model_id
        
        data = {
            "model": actual_model_id,
            "messages": [
                {
                    "role": "user",
                    "content": question
                }
            ],
            "max_tokens": 1000,
            "temperature": 0.7
        }
        
        try:
            print(f"  ğŸ”„ è™•ç†å•é¡Œ {question_num}/20...")
            response = requests.post(url, headers=headers, json=data, timeout=60)
            
            if response.status_code == 200:
                result = response.json()
                if 'choices' in result and len(result['choices']) > 0:
                    answer = result['choices'][0]['message']['content']
                    usage = result.get('usage', {})
                    
                    print(f"  âœ… å•é¡Œ {question_num} å®Œæˆ (Token: {usage.get('total_tokens', 0)})")
                    return {
                        'success': True,
                        'answer': answer,
                        'usage': usage
                    }
                else:
                    print(f"  âŒ å•é¡Œ {question_num} - API å›æ‡‰æ ¼å¼ç•°å¸¸")
                    return {'success': False, 'error': 'Invalid response format'}
            else:
                print(f"  âŒ å•é¡Œ {question_num} - API éŒ¯èª¤: {response.status_code}")
                return {'success': False, 'error': f'HTTP {response.status_code}'}
                    
        except Exception as e:
            print(f"  âŒ å•é¡Œ {question_num} - ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def process_single_model(self, model_info):
        """è™•ç†å–®ä¸€æ¨¡å‹çš„æ‰€æœ‰å•é¡Œ"""
        model_id = model_info["id"]
        model_name = model_info["name"]
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        print(f"\nâ–¶ï¸â–¶ï¸â–¶ï¸ é–‹å§‹è™•ç†æ¨¡å‹: {model_name} ({model_id})")
        
        print(f"\nğŸ¤– é–‹å§‹è™•ç†æ¨¡å‹: {model_name}")
        print(f"ğŸ“ æ¨¡å‹ID: {model_id}")
        print("=" * 60)
        
        # å¦‚æœä¸æ˜¯å·²é©—è­‰çš„æ¨¡å‹ï¼Œå…ˆæ¸¬è©¦å¯ç”¨æ€§
        if model_info["status"] != "verified":
            if not self.test_model_availability(model_info):
                print(f"âŒ æ¨¡å‹ {model_name} ä¸å¯ç”¨ï¼Œè·³éè™•ç†")
                return False, 0, 0
        
        results = []
        successful_count = 0
        total_tokens = 0
        
        for i, question in enumerate(self.questions, 1):
            result = self.call_llm_api(model_info, question, i)
            results.append(result)
            
            if result['success']:
                successful_count += 1
                total_tokens += result.get('usage', {}).get('total_tokens', 0)
            
            # æ¯å€‹å•é¡Œä¹‹é–“æš«åœ 3 ç§’
            if i < len(self.questions):
                time.sleep(3)
        
        # å„²å­˜çµæœ
        self.save_model_results(model_id, model_name, results, timestamp)
        
        print(f"\nğŸ“Š {model_name} è™•ç†å®Œæˆ:")
        print(f"  âœ… æˆåŠŸ: {successful_count}/20 å€‹å•é¡Œ")
        print(f"  ğŸ“ˆ ç¸½Token: {total_tokens}")
        print("=" * 60)
        
        return True, successful_count, total_tokens
    
    def save_model_results(self, model_id, model_name, results, timestamp):
        """å„²å­˜å–®ä¸€æ¨¡å‹çš„çµæœ"""
        # æº–å‚™è³‡æ–™
        data = []
        
        for i, (question, result) in enumerate(zip(self.questions, results), 1):
            row = {
                'å•é¡Œç·¨è™Ÿ': i,
                'å•é¡Œ': question,
                'å›ç­”': result.get('answer', 'è™•ç†å¤±æ•—') if result['success'] else 'è™•ç†å¤±æ•—',
                'ç‹€æ…‹': 'æˆåŠŸ' if result['success'] else 'å¤±æ•—',
                'éŒ¯èª¤è¨Šæ¯': result.get('error', '') if not result['success'] else '',
                'è¼¸å…¥Token': result.get('usage', {}).get('prompt_tokens', 0) if result['success'] else 0,
                'è¼¸å‡ºToken': result.get('usage', {}).get('completion_tokens', 0) if result['success'] else 0,
                'ç¸½Token': result.get('usage', {}).get('total_tokens', 0) if result['success'] else 0
            }
            data.append(row)
        
        # ç”Ÿæˆå®‰å…¨çš„æª”æ¡ˆåç¨±
        safe_model_name = model_id.replace('/', '_').replace('\\', '_')
        
        # å„²å­˜ Excel
        df = pd.DataFrame(data)
        excel_filename = f"AQUASKY_AIQA_{safe_model_name}_{timestamp}.xlsx"
        excel_path = self.output_dir / excel_filename
        df.to_excel(excel_path, index=False, engine='openpyxl')
        
        # å„²å­˜ Markdown
        md_filename = f"AQUASKY_AIQA_{safe_model_name}_{timestamp}.md"
        md_path = self.output_dir / md_filename
        
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(f"# AQUASKY AIQA Monitor - {model_name} æ¸¬è©¦å ±å‘Š\n\n")
            f.write(f"**ç”Ÿæˆæ™‚é–“**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**æ¨¡å‹**: {model_id}\n")
            f.write(f"**æ¨¡å‹åç¨±**: {model_name}\n\n")
            
            # çµ±è¨ˆè³‡è¨Š
            successful_count = sum(1 for r in results if r['success'])
            total_input_tokens = sum(r.get('usage', {}).get('prompt_tokens', 0) for r in results if r['success'])
            total_output_tokens = sum(r.get('usage', {}).get('completion_tokens', 0) for r in results if r['success'])
            
            f.write(f"## çµ±è¨ˆè³‡è¨Š\n\n")
            f.write(f"- **æˆåŠŸå•é¡Œæ•¸**: {successful_count}/20\n")
            f.write(f"- **ç¸½è¼¸å…¥Token**: {total_input_tokens}\n")
            f.write(f"- **ç¸½è¼¸å‡ºToken**: {total_output_tokens}\n")
            f.write(f"- **ç¸½Token**: {total_input_tokens + total_output_tokens}\n\n")
            
            # å•é¡Œå’Œå›ç­”
            for i, (question, result) in enumerate(zip(self.questions, results), 1):
                f.write(f"## å•é¡Œ {i}\n\n")
                f.write(f"**å•é¡Œ**: {question}\n\n")
                if result['success']:
                    f.write(f"**å›ç­”**: {result['answer']}\n\n")
                    usage = result.get('usage', {})
                    f.write(f"**Tokenä½¿ç”¨**: è¼¸å…¥ {usage.get('prompt_tokens', 0)}, è¼¸å‡º {usage.get('completion_tokens', 0)}, ç¸½è¨ˆ {usage.get('total_tokens', 0)}\n\n")
                else:
                    f.write(f"**ç‹€æ…‹**: è™•ç†å¤±æ•—\n")
                    f.write(f"**éŒ¯èª¤**: {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}\n\n")
                f.write("---\n\n")
        
        print(f"  ğŸ“ çµæœå·²å„²å­˜:")
        print(f"    Excel: {excel_filename}")
        print(f"    Markdown: {md_filename}")
    
    def run_processing(self):
        """åŸ·è¡Œè™•ç†"""
        print("ğŸš€ AQUASKY AIQA Monitor - å¯ç”¨æ¨¡å‹è™•ç†ç³»çµ±")
        print("=" * 60)
        
        # è¼‰å…¥é…ç½®
        if not self.load_config():
            return False
        
        # æå–å•é¡Œ
        if not self.extract_questions():
            return False
        
        print(f"\nğŸ“‹ å°‡ä¾åºè™•ç†ä»¥ä¸‹æ¨¡å‹:")
        for i, model in enumerate(self.working_models, 1):
            status_icon = "âœ…" if model["status"] == "verified" else "ğŸ§ª"
            print(f"  {i}. {status_icon} {model['name']} ({model['id']})")
        
        print(f"\nğŸ’¡ æ¯å€‹æ¨¡å‹å°‡è™•ç† 20 å€‹å•é¡Œï¼Œå®Œæˆå¾Œè‡ªå‹•å„²å­˜çµæœ")
        print("=" * 60)
        
        # é–‹å§‹è™•ç†æ¯å€‹æ¨¡å‹
        total_successful = 0
        total_tokens = 0
        processed_models = 0
        
        for i, model_info in enumerate(self.working_models, 1):
            print(f"\nğŸ”„ é€²åº¦: {i}/{len(self.working_models)}")
            
            try:
                success, successful, tokens = self.process_single_model(model_info)
                
                if success:
                    processed_models += 1
                    total_successful += successful
                    total_tokens += tokens
                
                # æ¨¡å‹ä¹‹é–“æš«åœ 5 ç§’
                if i < len(self.working_models):
                    print(f"â¸ï¸ æš«åœ 5 ç§’å¾Œè™•ç†ä¸‹ä¸€å€‹æ¨¡å‹...")
                    time.sleep(5)
                    
            except KeyboardInterrupt:
                print(f"\nâ¸ï¸ ä½¿ç”¨è€…ä¸­æ–·è™•ç†ï¼Œå·²å®Œæˆ {processed_models} å€‹æ¨¡å‹")
                break
            except Exception as e:
                print(f"\nâŒ è™•ç†æ¨¡å‹ {model_info['name']} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                continue
        
        # é¡¯ç¤ºç¸½çµ
        print(f"\nğŸ‰ è™•ç†å®Œæˆï¼")
        print(f"ğŸ“Š æˆåŠŸè™•ç†æ¨¡å‹æ•¸: {processed_models}")
        print(f"ğŸ“ˆ ç¸½æˆåŠŸå•é¡Œæ•¸: {total_successful}")
        print(f"ğŸ’° ç¸½Tokenä½¿ç”¨: {total_tokens}")
        print(f"ğŸ“ æ‰€æœ‰çµæœæª”æ¡ˆå·²å„²å­˜è‡³ outputs/ ç›®éŒ„")
        print("=" * 60)
        
        return True

def main():
    """ä¸»ç¨‹å¼"""
    print("===== é–‹å§‹åŸ·è¡Œ working_models_processor.py =====")
    try:
        processor = WorkingModelsProcessor()
        processor.run_processing()
    except Exception as e:
        import traceback
        print(f"\nâŒâŒâŒ ç™¼ç”Ÿæœªæ•ç²çš„éŒ¯èª¤: {str(e)}")
        print("è©³ç´°éŒ¯èª¤è¿½è¹¤:")
        traceback.print_exc()
    print("===== è…³æœ¬åŸ·è¡ŒçµæŸ =====")

if __name__ == "__main__":
    main()
