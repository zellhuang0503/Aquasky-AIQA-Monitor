#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AQUASKY AIQA Monitor - é †åºè™•ç† LLM è…³æœ¬
æ¯æ¬¡åªè™•ç†ä¸€å€‹ LLM æ¨¡å‹çš„ 20 å€‹å•é¡Œï¼Œå®Œæˆä¸¦å„²å­˜å¾Œå†é€²è¡Œä¸‹ä¸€å€‹æ¨¡å‹
é¿å… API éæ–¼é »ç¹ï¼Œæä¾›æ›´å¥½çš„æ§åˆ¶å’Œç©©å®šæ€§
"""

import os
import sys
import json
import requests
import configparser
import time
from datetime import datetime
from pathlib import Path
import pandas as pd

class SequentialLLMProcessor:
    """é †åºè™•ç† LLM çš„è™•ç†å™¨"""
    
    def __init__(self):
        self.project_root = Path.cwd()
        self.output_dir = self.project_root / "outputs"
        self.output_dir.mkdir(exist_ok=True)
        
        # æ”¯æ´çš„ LLM æ¨¡å‹æ¸…å–®ï¼ˆå·²é©—è­‰å¯ç”¨ï¼‰
        self.available_models = [
            "deepseek/deepseek-chat",           # DeepSeek Chat
            "anthropic/claude-3.5-sonnet",     # Claude 3.5 Sonnet
            "openai/gpt-4o-mini",              # GPT-4o Mini
            "google/gemini-flash-1.5",         # Gemini Flash 1.5
            "meta-llama/llama-3.1-8b-instruct", # Llama 3.1 8B
            "mistralai/mistral-7b-instruct",   # Mistral 7B
            "perplexity/sonar-pro",            # Perplexity Sonar Pro
            "x-ai/grok-3-mini-beta"            # Grok 3 Mini Beta (âœ… å·²é©—è­‰)
        ]
        
        # æ¨¡å‹é¡¯ç¤ºåç¨±
        self.model_display_names = {
            "deepseek/deepseek-chat": "DeepSeek Chat",
            "anthropic/claude-3.5-sonnet": "Claude 3.5 Sonnet",
            "openai/gpt-4o-mini": "GPT-4o Mini",
            "google/gemini-flash-1.5": "Gemini Flash 1.5",
            "meta-llama/llama-3.1-8b-instruct": "Llama 3.1 8B",
            "mistralai/mistral-7b-instruct": "Mistral 7B",
            "perplexity/sonar-pro": "Perplexity Sonar Pro",
            "x-ai/grok-4": "Grok 4",
            "x-ai/grok-3-mini-beta": "Grok 3 Mini Beta"
        }
        
        self.api_key = None
        self.questions = []
        
    def load_config(self):
        """è¼‰å…¥é…ç½®æª”æ¡ˆ"""
        config = configparser.ConfigParser()
        config_path = Path("config.ini")
        
        if not config_path.exists():
            print("âŒ æ‰¾ä¸åˆ° config.ini æª”æ¡ˆ")
            return False
        
        config.read(config_path, encoding='utf-8')
        self.api_key = config.get('api_keys', 'OPENROUTER_API_KEY', fallback=None)
        
        if not self.api_key or self.api_key == 'your_openrouter_api_key_here':
            print("âŒ è«‹åœ¨ config.ini ä¸­è¨­å®šæœ‰æ•ˆçš„ OPENROUTER_API_KEY")
            return False
        
        return True
    
    def extract_questions(self):
        """å¾å•é¡Œæª”æ¡ˆä¸­æå–æ‰€æœ‰ 20 å€‹å•é¡Œ"""
        questions_file = Path("AQUASKY AEO ç›£æ§å°ˆæ¡ˆ - é»ƒé‡‘å•é¡Œåº« V2.0.md")
        
        if not questions_file.exists():
            print(f"âŒ æ‰¾ä¸åˆ°å•é¡Œæª”æ¡ˆ: {questions_file}")
            return False
        
        try:
            with open(questions_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æ‰‹å‹•æå–æ‰€æœ‰ 20 å€‹å•é¡Œ
            questions = []
            lines = content.split('\n')
            
            for line in lines:
                line = line.strip()
                # å°‹æ‰¾ä»¥æ•¸å­—é–‹é ­çš„å•é¡Œè¡Œ
                if line and any(line.startswith(f"{i}.") for i in range(1, 21)):
                    # ç§»é™¤å•é¡Œç·¨è™Ÿï¼Œåªä¿ç•™å•é¡Œå…§å®¹
                    question = line.split('.', 1)[1].strip()
                    # ç§»é™¤è‹±æ–‡ç¿»è­¯éƒ¨åˆ†ï¼ˆæ‹¬è™Ÿå…§å®¹ï¼‰
                    if '(' in question and ')' in question:
                        question = question.split('(')[0].strip()
                    questions.append(question)
            
            self.questions = questions
            print(f"âœ… æˆåŠŸæå– {len(questions)} å€‹å•é¡Œ")
            return len(questions) == 20
            
        except Exception as e:
            print(f"âŒ è®€å–å•é¡Œæª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return False
    
    def call_llm_api(self, model, question, question_num):
        """å‘¼å« LLM API"""
        url = "https://openrouter.ai/api/v1/chat/completions"
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://github.com/aquasky-aiqa-monitor",
            "X-Title": "AQUASKY AIQA Monitor"
        }
        
        data = {
            "model": model,
            "messages": [
                {
                    "role": "user",
                    "content": question
                }
            ],
            "max_tokens": 1000,
            "temperature": 0.7
        }
        
        try:
            print(f"  ğŸ”„ è™•ç†å•é¡Œ {question_num}/20: {question[:50]}...")
            response = requests.post(url, headers=headers, json=data, timeout=60)
            
            if response.status_code == 200:
                result = response.json()
                if 'choices' in result and len(result['choices']) > 0:
                    answer = result['choices'][0]['message']['content']
                    usage = result.get('usage', {})
                    
                    print(f"  âœ… å•é¡Œ {question_num} å®Œæˆ (Token: {usage.get('total_tokens', 0)})")
                    return {
                        'success': True,
                        'answer': answer,
                        'usage': usage
                    }
                else:
                    print(f"  âŒ å•é¡Œ {question_num} - API å›æ‡‰æ ¼å¼ç•°å¸¸")
                    return {'success': False, 'error': 'Invalid response format'}
            else:
                print(f"  âŒ å•é¡Œ {question_num} - API éŒ¯èª¤: {response.status_code}")
                try:
                    error_info = response.json()
                    return {'success': False, 'error': f'HTTP {response.status_code}: {error_info}'}
                except:
                    return {'success': False, 'error': f'HTTP {response.status_code}: {response.text}'}
                    
        except Exception as e:
            print(f"  âŒ å•é¡Œ {question_num} - ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def process_single_model(self, model):
        """è™•ç†å–®ä¸€æ¨¡å‹çš„æ‰€æœ‰å•é¡Œ"""
        model_name = self.model_display_names.get(model, model)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        print(f"\nğŸ¤– é–‹å§‹è™•ç†æ¨¡å‹: {model_name}")
        print(f"ğŸ“ æ¨¡å‹ID: {model}")
        print("=" * 60)
        
        results = []
        successful_count = 0
        total_tokens = 0
        
        for i, question in enumerate(self.questions, 1):
            result = self.call_llm_api(model, question, i)
            results.append(result)
            
            if result['success']:
                successful_count += 1
                total_tokens += result.get('usage', {}).get('total_tokens', 0)
            
            # æ¯å€‹å•é¡Œä¹‹é–“æš«åœ 3 ç§’ï¼Œé¿å… API é™åˆ¶
            if i < len(self.questions):
                time.sleep(3)
        
        # å„²å­˜çµæœ
        self.save_model_results(model, model_name, results, timestamp)
        
        print(f"\nğŸ“Š {model_name} è™•ç†å®Œæˆ:")
        print(f"  âœ… æˆåŠŸ: {successful_count}/20 å€‹å•é¡Œ")
        print(f"  ğŸ“ˆ ç¸½Token: {total_tokens}")
        print("=" * 60)
        
        return successful_count, total_tokens
    
    def save_model_results(self, model, model_name, results, timestamp):
        """å„²å­˜å–®ä¸€æ¨¡å‹çš„çµæœ"""
        # æº–å‚™è³‡æ–™
        data = []
        
        for i, (question, result) in enumerate(zip(self.questions, results), 1):
            row = {
                'å•é¡Œç·¨è™Ÿ': i,
                'å•é¡Œ': question,
                'å›ç­”': result.get('answer', 'è™•ç†å¤±æ•—') if result['success'] else 'è™•ç†å¤±æ•—',
                'ç‹€æ…‹': 'æˆåŠŸ' if result['success'] else 'å¤±æ•—',
                'éŒ¯èª¤è¨Šæ¯': result.get('error', '') if not result['success'] else '',
                'è¼¸å…¥Token': result.get('usage', {}).get('prompt_tokens', 0) if result['success'] else 0,
                'è¼¸å‡ºToken': result.get('usage', {}).get('completion_tokens', 0) if result['success'] else 0,
                'ç¸½Token': result.get('usage', {}).get('total_tokens', 0) if result['success'] else 0
            }
            data.append(row)
        
        # ç”Ÿæˆæª”æ¡ˆåç¨±ï¼ˆä½¿ç”¨å®‰å…¨çš„æª”æ¡ˆåç¨±ï¼‰
        safe_model_name = model.replace('/', '_').replace('\\', '_')
        
        # å„²å­˜ Excel
        df = pd.DataFrame(data)
        excel_filename = f"AQUASKY_AIQA_{safe_model_name}_{timestamp}.xlsx"
        excel_path = self.output_dir / excel_filename
        df.to_excel(excel_path, index=False, engine='openpyxl')
        
        # å„²å­˜ Markdown
        md_filename = f"AQUASKY_AIQA_{safe_model_name}_{timestamp}.md"
        md_path = self.output_dir / md_filename
        
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(f"# AQUASKY AIQA Monitor - {model_name} æ¸¬è©¦å ±å‘Š\n\n")
            f.write(f"**ç”Ÿæˆæ™‚é–“**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**æ¨¡å‹**: {model}\n")
            f.write(f"**æ¨¡å‹åç¨±**: {model_name}\n\n")
            
            # çµ±è¨ˆè³‡è¨Š
            successful_count = sum(1 for r in results if r['success'])
            total_input_tokens = sum(r.get('usage', {}).get('prompt_tokens', 0) for r in results if r['success'])
            total_output_tokens = sum(r.get('usage', {}).get('completion_tokens', 0) for r in results if r['success'])
            
            f.write(f"## çµ±è¨ˆè³‡è¨Š\n\n")
            f.write(f"- **æˆåŠŸå•é¡Œæ•¸**: {successful_count}/20\n")
            f.write(f"- **ç¸½è¼¸å…¥Token**: {total_input_tokens}\n")
            f.write(f"- **ç¸½è¼¸å‡ºToken**: {total_output_tokens}\n")
            f.write(f"- **ç¸½Token**: {total_input_tokens + total_output_tokens}\n\n")
            
            # å•é¡Œå’Œå›ç­”
            for i, (question, result) in enumerate(zip(self.questions, results), 1):
                f.write(f"## å•é¡Œ {i}\n\n")
                f.write(f"**å•é¡Œ**: {question}\n\n")
                if result['success']:
                    f.write(f"**å›ç­”**: {result['answer']}\n\n")
                    usage = result.get('usage', {})
                    f.write(f"**Tokenä½¿ç”¨**: è¼¸å…¥ {usage.get('prompt_tokens', 0)}, è¼¸å‡º {usage.get('completion_tokens', 0)}, ç¸½è¨ˆ {usage.get('total_tokens', 0)}\n\n")
                else:
                    f.write(f"**ç‹€æ…‹**: è™•ç†å¤±æ•—\n")
                    f.write(f"**éŒ¯èª¤**: {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}\n\n")
                f.write("---\n\n")
        
        print(f"  ğŸ“ çµæœå·²å„²å­˜:")
        print(f"    Excel: {excel_filename}")
        print(f"    Markdown: {md_filename}")
    
    def run_sequential_processing(self):
        """åŸ·è¡Œé †åºè™•ç†"""
        print("ğŸš€ AQUASKY AIQA Monitor - é †åº LLM è™•ç†ç³»çµ±")
        print("=" * 60)
        
        # è¼‰å…¥é…ç½®
        if not self.load_config():
            return False
        
        # æå–å•é¡Œ
        if not self.extract_questions():
            return False
        
        print(f"\nğŸ“‹ å°‡ä¾åºè™•ç† {len(self.available_models)} å€‹ LLM æ¨¡å‹:")
        for i, model in enumerate(self.available_models, 1):
            model_name = self.model_display_names.get(model, model)
            print(f"  {i}. {model_name} ({model})")
        
        print(f"\nğŸ’¡ æ¯å€‹æ¨¡å‹å°‡è™•ç† 20 å€‹å•é¡Œï¼Œå®Œæˆå¾Œè‡ªå‹•å„²å­˜çµæœ")
        print(f"â±ï¸ é ä¼°ç¸½æ™‚é–“: ç´„ {len(self.available_models) * 20 * 3 / 60:.0f} åˆ†é˜")
        print("=" * 60)
        
        # è©¢å•æ˜¯å¦é–‹å§‹
        try:
            confirm = input("\næ˜¯å¦é–‹å§‹é †åºè™•ç†ï¼Ÿ(y/n): ").strip().lower()
            if confirm != 'y':
                print("âŒ ä½¿ç”¨è€…å–æ¶ˆè™•ç†")
                return False
        except KeyboardInterrupt:
            print("\nâŒ ä½¿ç”¨è€…ä¸­æ–·è™•ç†")
            return False
        
        # é–‹å§‹è™•ç†æ¯å€‹æ¨¡å‹
        total_successful = 0
        total_tokens = 0
        
        for i, model in enumerate(self.available_models, 1):
            print(f"\nğŸ”„ é€²åº¦: {i}/{len(self.available_models)}")
            
            try:
                successful, tokens = self.process_single_model(model)
                total_successful += successful
                total_tokens += tokens
                
                # æ¨¡å‹ä¹‹é–“æš«åœ 5 ç§’
                if i < len(self.available_models):
                    print(f"â¸ï¸ æš«åœ 5 ç§’å¾Œè™•ç†ä¸‹ä¸€å€‹æ¨¡å‹...")
                    time.sleep(5)
                    
            except KeyboardInterrupt:
                print(f"\nâ¸ï¸ ä½¿ç”¨è€…ä¸­æ–·è™•ç†ï¼Œå·²å®Œæˆ {i-1} å€‹æ¨¡å‹")
                break
            except Exception as e:
                print(f"\nâŒ è™•ç†æ¨¡å‹ {model} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                continue
        
        # é¡¯ç¤ºç¸½çµ
        print(f"\nğŸ‰ é †åºè™•ç†å®Œæˆï¼")
        print(f"ğŸ“Š ç¸½æˆåŠŸå•é¡Œæ•¸: {total_successful}")
        print(f"ğŸ“ˆ ç¸½Tokenä½¿ç”¨: {total_tokens}")
        print(f"ğŸ“ æ‰€æœ‰çµæœæª”æ¡ˆå·²å„²å­˜è‡³ outputs/ ç›®éŒ„")
        print("=" * 60)
        
        return True

def main():
    """ä¸»ç¨‹å¼"""
    processor = SequentialLLMProcessor()
    processor.run_sequential_processing()

if __name__ == "__main__":
    main()
