#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AQUASKY AEO äº¤å‰åˆ†æå ±å‘Šç”Ÿæˆå™¨
ç¢ºä¿æ¯æ¬¡åˆ†æéƒ½é”åˆ°æ¨™æº–åŒ–æ°´æº–
"""

import pandas as pd
from pathlib import Path
from datetime import datetime
import json
import re
from typing import Dict, List, Tuple, Any

class CrossAnalysisGenerator:
    """äº¤å‰åˆ†æå ±å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.project_root = Path.cwd()
        self.output_dir = self.project_root / "outputs"
        self.cross_analysis_dir = self.output_dir / "cross_analysis"
        self.templates_dir = self.project_root / "templates"
        self.cross_analysis_dir.mkdir(exist_ok=True)
        
        # æ¨¡å‹åç¨±æ˜ å°„
        self.model_names = {
            'anthropic_claude-3.5-sonnet': 'Claude 3.5 Sonnet',
            'deepseek_deepseek-v3.1-base': 'DeepSeek v3.1 Base',
            'google_gemini-flash-1.5': 'Gemini Flash 1.5',
            'meta-llama_llama-3.1-8b-instruct': 'Llama 3.1 8B',
            'mistralai_mistral-7b-instruct': 'Mistral 7B',
            'openai_gpt-5-mini': 'GPT-5 Mini',
            'perplexity_sonar-pro': 'Perplexity Sonar Pro',
            'x-ai_grok-3-mini-beta': 'Grok 3 Mini Beta'
        }
        
        # åˆ†æé—œéµè©
        self.brand_keywords = ['aquasky', 'aqua sky']
        self.website_keywords = ['aquaskyplus.com', 'aquasky.com']
        self.specific_keywords = ['å…·é«”', 'è©³ç´°', 'å°ˆæ¥­', 'æŠ€è¡“', 'è¦æ ¼', 'èªè­‰', 'æ¸¬è©¦', 'å“è³ª', 'æµç¨‹', 'æ¨™æº–']
        self.generic_keywords = ['ç„¡æ³•ç¢ºèª', 'å»ºè­°è¯ç¹«', 'è«‹è¯çµ¡', 'ç„¡æ³•æä¾›', 'ä¸ç¢ºå®š', 'ç„¡ç›¸é—œè³‡è¨Š', 'å»ºè­°æŸ¥è©¢']
    
    def load_reports(self, date_pattern: str) -> Dict[str, pd.DataFrame]:
        """è¼‰å…¥æŒ‡å®šæ—¥æœŸçš„æ‰€æœ‰å ±å‘Š"""
        all_data = {}
        
        # å°‹æ‰¾åŒ¹é…çš„æª”æ¡ˆ
        pattern = f"*{date_pattern}*.xlsx"
        files = list(self.output_dir.glob(pattern))
        
        for file_path in files:
            try:
                df = pd.read_excel(file_path)
                # å¾æª”åæå–æ¨¡å‹key
                filename = file_path.name
                parts = filename.split('_')
                if len(parts) >= 4:
                    model_key = '_'.join(parts[2:4])
                    all_data[model_key] = df
                    print(f'âœ… è¼‰å…¥: {self.model_names.get(model_key, model_key)}')
            except Exception as e:
                print(f'âŒ è¼‰å…¥å¤±æ•— {file_path.name}: {e}')
        
        return all_data
    
    def analyze_answer_quality(self, answer: str) -> Dict[str, bool]:
        """åˆ†æå–®å€‹å›ç­”çš„å“è³ªæŒ‡æ¨™"""
        if pd.isna(answer) or answer == 'è™•ç†å¤±æ•—':
            return {
                'has_brand_mention': False,
                'has_website_mention': False,
                'has_specific_info': False,
                'is_generic_response': True
            }
        
        answer_lower = str(answer).lower()
        
        return {
            'has_brand_mention': any(keyword in answer_lower for keyword in self.brand_keywords),
            'has_website_mention': any(keyword in answer_lower for keyword in self.website_keywords),
            'has_specific_info': any(keyword in answer_lower for keyword in self.specific_keywords),
            'is_generic_response': any(keyword in answer_lower for keyword in self.generic_keywords)
        }
    
    def calculate_model_metrics(self, df: pd.DataFrame) -> Dict[str, Any]:
        """è¨ˆç®—å–®å€‹æ¨¡å‹çš„æŒ‡æ¨™"""
        success_count = len(df[df['ç‹€æ…‹'] == 'æˆåŠŸ'])
        total_tokens = df['ç¸½Token'].sum()
        avg_tokens = total_tokens / len(df) if len(df) > 0 else 0
        
        # åˆ†æå›ç­”å“è³ª
        brand_mentions = 0
        website_mentions = 0
        specific_info = 0
        generic_responses = 0
        
        for _, row in df.iterrows():
            if row['ç‹€æ…‹'] == 'æˆåŠŸ':
                quality = self.analyze_answer_quality(row['å›ç­”'])
                if quality['has_brand_mention']:
                    brand_mentions += 1
                if quality['has_website_mention']:
                    website_mentions += 1
                if quality['has_specific_info']:
                    specific_info += 1
                if quality['is_generic_response']:
                    generic_responses += 1
        
        return {
            'success_count': success_count,
            'total_tokens': total_tokens,
            'avg_tokens': avg_tokens,
            'brand_mention_rate': (brand_mentions / len(df)) * 100,
            'website_mention_rate': (website_mentions / len(df)) * 100,
            'specific_info_rate': (specific_info / len(df)) * 100,
            'generic_response_rate': (generic_responses / len(df)) * 100
        }
    
    def rank_models(self, all_metrics: Dict[str, Dict[str, Any]]) -> Dict[str, List[Tuple[str, Dict[str, Any]]]]:
        """å°æ¨¡å‹é€²è¡Œå„ç¨®æ’å"""
        models_list = [(k, v) for k, v in all_metrics.items()]
        
        return {
            'efficiency': sorted(models_list, key=lambda x: x[1]['total_tokens']),
            'quality': sorted(models_list, key=lambda x: (
                x[1]['brand_mention_rate'] + 
                x[1]['website_mention_rate'] + 
                x[1]['specific_info_rate'] - 
                x[1]['generic_response_rate']
            ), reverse=True),
            'brand_recognition': sorted(models_list, key=lambda x: x[1]['brand_mention_rate'], reverse=True)
        }
    
    def analyze_questions(self, all_data: Dict[str, pd.DataFrame], sample_questions: int = 5) -> str:
        """åˆ†æé—œéµå•é¡Œçš„å›ç­”å·®ç•°"""
        analysis_text = ""
        
        for i in range(1, min(sample_questions + 1, 21)):
            analysis_text += f"### å•é¡Œ {i}\n\n"
            
            # ç²å–å•é¡Œå…§å®¹
            first_model = list(all_data.keys())[0]
            if i <= len(all_data[first_model]):
                question = all_data[first_model].iloc[i-1]['å•é¡Œ']
                analysis_text += f"**å•é¡Œ**: {question}\n\n"
                
                # æ”¶é›†æ‰€æœ‰å›ç­”çš„å“è³ªæŒ‡æ¨™
                model_qualities = {}
                for model_key, df in all_data.items():
                    if i <= len(df):
                        row = df.iloc[i-1]
                        if row['ç‹€æ…‹'] == 'æˆåŠŸ':
                            quality = self.analyze_answer_quality(row['å›ç­”'])
                            model_qualities[model_key] = {
                                'answer': str(row['å›ç­”']),
                                'tokens': row['ç¸½Token'],
                                'length': len(str(row['å›ç­”'])),
                                **quality
                            }
                
                # å“è³ªåˆ†æ
                analysis_text += "**å›ç­”å“è³ªè©•ä¼°**:\n"
                
                brand_mentions = [self.model_names.get(k, k) for k, v in model_qualities.items() if v['has_brand_mention']]
                website_mentions = [self.model_names.get(k, k) for k, v in model_qualities.items() if v['has_website_mention']]
                specific_info = [self.model_names.get(k, k) for k, v in model_qualities.items() if v['has_specific_info']]
                generic_responses = [self.model_names.get(k, k) for k, v in model_qualities.items() if v['is_generic_response']]
                
                if brand_mentions:
                    analysis_text += f"- âœ… **æåŠAQUASKYå“ç‰Œ**: {', '.join(brand_mentions)}\n"
                else:
                    analysis_text += "- âŒ **ç„¡æ¨¡å‹æåŠAQUASKYå“ç‰Œ**\n"
                
                if website_mentions:
                    analysis_text += f"- âœ… **æåŠå®˜ç¶²**: {', '.join(website_mentions)}\n"
                else:
                    analysis_text += "- âŒ **ç„¡æ¨¡å‹æåŠå®˜ç¶²**\n"
                
                if specific_info:
                    analysis_text += f"- âœ… **æä¾›å…·é«”è³‡è¨Š**: {', '.join(specific_info)}\n"
                else:
                    analysis_text += "- âŒ **ç¼ºä¹å…·é«”è³‡è¨Š**\n"
                
                if generic_responses:
                    analysis_text += f"- âš ï¸ **æ³›åŒ–å›ç­”**: {', '.join(generic_responses)}\n"
                
                # æ¨è–¦æœ€ä½³å›ç­”
                if model_qualities:
                    best_model = max(model_qualities.items(), key=lambda x: (
                        x[1]['has_brand_mention'] * 3 +
                        x[1]['has_website_mention'] * 2 +
                        x[1]['has_specific_info'] * 1 -
                        x[1]['is_generic_response'] * 2
                    ))
                    analysis_text += f"\n**æœ€ä½³å›ç­”**: {self.model_names.get(best_model[0], best_model[0])}\n"
                
                analysis_text += "\n---\n\n"
        
        return analysis_text
    
    def generate_report(self, date_pattern: str) -> str:
        """ç”Ÿæˆå®Œæ•´çš„äº¤å‰åˆ†æå ±å‘Š"""
        print(f"ğŸš€ é–‹å§‹ç”Ÿæˆ {date_pattern} çš„äº¤å‰åˆ†æå ±å‘Š...")
        
        # è¼‰å…¥æ•¸æ“š
        all_data = self.load_reports(date_pattern)
        if not all_data:
            raise ValueError(f"æ‰¾ä¸åˆ° {date_pattern} çš„å ±å‘Šæª”æ¡ˆ")
        
        print(f"ğŸ“Š æˆåŠŸè¼‰å…¥ {len(all_data)} å€‹æ¨¡å‹å ±å‘Š")
        
        # è¨ˆç®—å„æ¨¡å‹æŒ‡æ¨™
        all_metrics = {}
        for model_key, df in all_data.items():
            all_metrics[model_key] = self.calculate_model_metrics(df)
        
        # æ’å
        rankings = self.rank_models(all_metrics)
        
        # è¨ˆç®—æ•´é«”æŒ‡æ¨™
        total_questions = len(list(all_data.values())[0])
        overall_brand_rate = sum(m['brand_mention_rate'] for m in all_metrics.values()) / len(all_metrics)
        overall_website_rate = sum(m['website_mention_rate'] for m in all_metrics.values()) / len(all_metrics)
        overall_specific_rate = sum(m['specific_info_rate'] for m in all_metrics.values()) / len(all_metrics)
        overall_generic_rate = sum(m['generic_response_rate'] for m in all_metrics.values()) / len(all_metrics)
        
        # ç”Ÿæˆå ±å‘Šå…§å®¹
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        report_content = f"""# AQUASKY AEO ç›£æ§å°ˆæ¡ˆ - è©³ç´°äº¤å‰åˆ†æå ±å‘Š

**ç”Ÿæˆæ™‚é–“**: {timestamp}
**åˆ†ææ¨¡å‹æ•¸**: {len(all_data)}
**åˆ†æä¾æ“š**: {date_pattern} åŸ·è¡Œçš„ {total_questions} å€‹é»ƒé‡‘å•é¡Œæ¸¬è©¦çµæœ

## åŸ·è¡Œæ‘˜è¦

æœ¬æ¬¡åˆ†æç™¼ç¾æ‰€æœ‰ {len(all_data)} å€‹ LLM æ¨¡å‹å° AQUASKY ç›¸é—œå•é¡Œçš„å›ç­”å“è³ªæ™®éåä½ï¼Œä¸»è¦å•é¡ŒåŒ…æ‹¬ï¼š
- **å“ç‰Œè­˜åˆ¥åº¦ä½**: å¹³å‡åªæœ‰ {overall_brand_rate:.1f}% çš„å›ç­”æåŠ AQUASKY å“ç‰Œ
- **ç¼ºä¹å…·é«”è³‡è¨Š**: å¹³å‡åªæœ‰ {overall_specific_rate:.1f}% çš„å›ç­”æä¾›å…·é«”è³‡è¨Š
- **ç„¡å®˜ç¶²å¼•ç”¨**: å¹³å‡åªæœ‰ {overall_website_rate:.1f}% çš„å›ç­”å¼•ç”¨å®˜ç¶²
- **æ³›åŒ–å›ç­”ç‡é«˜**: {overall_generic_rate:.1f}% çš„å›ç­”ç‚ºæ³›åŒ–å»ºè­°

## æ¨¡å‹æ•ˆèƒ½æ¯”è¼ƒ

### Token ä½¿ç”¨æ•ˆç‡æ’å
"""
        
        for i, (model_key, metrics) in enumerate(rankings['efficiency'], 1):
            model_name = self.model_names.get(model_key, model_key)
            report_content += f"{i}. **{model_name}**: {metrics['success_count']}/20 æˆåŠŸ, ç¸½Token: {metrics['total_tokens']:,}, å¹³å‡: {metrics['avg_tokens']:.0f}\n"
        
        report_content += "\n### å›ç­”å“è³ªæ’å\n"
        for i, (model_key, metrics) in enumerate(rankings['quality'], 1):
            model_name = self.model_names.get(model_key, model_key)
            quality_score = (metrics['brand_mention_rate'] + metrics['website_mention_rate'] + 
                           metrics['specific_info_rate'] - metrics['generic_response_rate'])
            report_content += f"{i}. **{model_name}**: å“è³ªåˆ†æ•¸ {quality_score:.1f}, å“ç‰ŒæåŠ {metrics['brand_mention_rate']:.1f}%\n"
        
        # é—œéµå•é¡Œåˆ†æ
        report_content += "\n## é—œéµå•é¡Œåˆ†æ\n\n"
        report_content += self.analyze_questions(all_data)
        
        # æ¨¡å‹è¡¨ç¾è©•ç´š
        best_overall = rankings['quality'][0]
        best_efficiency = rankings['efficiency'][0]
        best_value = rankings['efficiency'][1] if len(rankings['efficiency']) > 1 else rankings['efficiency'][0]
        
        report_content += f"""## æ¨¡å‹è¡¨ç¾è©•ç´š

### ğŸ† æœ€ä½³ç¶œåˆè¡¨ç¾
**{self.model_names.get(best_overall[0], best_overall[0])}**
- å„ªé»: å“è³ªåˆ†æ•¸æœ€é«˜ï¼Œå›ç­”ç›¸å°å®Œæ•´
- ç¼ºé»: ä»ç¼ºä¹ AQUASKY ç‰¹å®šè³‡è¨Š

### ğŸ¥ˆ æœ€ä½³æ•ˆç‡è¡¨ç¾  
**{self.model_names.get(best_efficiency[0], best_efficiency[0])}**
- å„ªé»: Token ä½¿ç”¨æœ€å°‘ ({best_efficiency[1]['total_tokens']:,})
- ç¼ºé»: å›ç­”å¯èƒ½è¼ƒç°¡çŸ­

### ğŸ¥‰ æœ€ä½³æ€§åƒ¹æ¯”
**{self.model_names.get(best_value[0], best_value[0])}**
- å„ªé»: æ•ˆç‡èˆ‡å“è³ªå¹³è¡¡
- ç¼ºé»: åŒæ¨£ç¼ºä¹å“ç‰Œç‰¹å®šè³‡è¨Š

## æ ¹æœ¬å•é¡Œè¨ºæ–·

### 1. å…§å®¹å¯ç™¼ç¾æ€§å•é¡Œ
- AQUASKY å®˜ç¶²å…§å®¹æœªè¢« LLM æœ‰æ•ˆç´¢å¼•
- ç¼ºä¹çµæ§‹åŒ–çš„ç”¢å“è³‡è¨Šé é¢
- FAQ é é¢ä¸å­˜åœ¨æˆ–ä¸å®Œæ•´

### 2. SEO èˆ‡ AEO å„ªåŒ–ä¸è¶³
- ç¼ºä¹é‡å°ã€Œé»ƒé‡‘å•é¡Œã€çš„å°ˆé–€å…§å®¹é 
- æ²’æœ‰ Schema.org çµæ§‹åŒ–æ¨™è¨˜
- é—œéµè³‡è¨Šåˆ†æ•£ï¼Œé›£ä»¥è¢« AI æå–

### 3. å“ç‰Œæ¬Šå¨æ€§å»ºç«‹ä¸è¶³
- ç¼ºä¹ç¬¬ä¸‰æ–¹æ¬Šå¨ä¾†æºçš„å“ç‰ŒæåŠ
- ç”¢å“è¦æ ¼ã€èªè­‰è³‡è¨Šæœªç³»çµ±åŒ–å‘ˆç¾
- å…¬å¸èƒŒæ™¯ã€æŠ€è¡“å„ªå‹¢æè¿°ä¸æ¸…æ™°

## ç¶²ç«™å…§å®¹æ”¹å–„å»ºè­°

### ç«‹å³åŸ·è¡Œ (1-2 é€±)

#### A. å»ºç«‹æ ¸å¿ƒè³‡è¨Šé é¢
1. **ç”¢å“ç·šç¸½è¦½é ** (`/products/overview`)
   - åˆ—å‡ºæ‰€æœ‰ä¸»è¦ç”¢å“é¡åˆ¥
   - æ¯å€‹ç”¢å“çš„æ ¸å¿ƒè¦æ ¼èˆ‡æ‡‰ç”¨
   - æ·»åŠ  Product Schema æ¨™è¨˜

2. **å“è³ªæ§åˆ¶é ** (`/quality/control-process`)
   - è©³è¿° AQUASKY ç‰¹æœ‰çš„ QC æµç¨‹
   - åŒ…å«æ¸¬è©¦è¨­å‚™ã€æ¨™æº–ã€èªè­‰æµç¨‹
   - æ·»åŠ å…·é«”çš„æ¸¬è©¦æ•¸æ“šèˆ‡æ¡ˆä¾‹

3. **èªè­‰èˆ‡è³‡è³ªé ** (`/certifications`)
   - åˆ—å‡ºæ‰€æœ‰åœ‹éš›èªè­‰ (NSF, CE, WRAS, ISO ç­‰)
   - æä¾›èªè­‰è­‰æ›¸åœ–ç‰‡èˆ‡ç·¨è™Ÿ
   - èªªæ˜å„èªè­‰çš„æ„ç¾©èˆ‡é©ç”¨ç¯„åœ

#### B. FAQ é é¢å»ºè¨­
å»ºç«‹é‡å° 20 å€‹é»ƒé‡‘å•é¡Œçš„å°ˆé–€ FAQ é é¢ï¼š
- ä½¿ç”¨ FAQPage Schema æ¨™è¨˜
- æ¯å€‹å•ç­”éƒ½è¦æœ‰å…·é«”ã€æ¬Šå¨çš„ç­”æ¡ˆ
- åŒ…å«å…§éƒ¨é€£çµåˆ°ç›¸é—œç”¢å“é é¢

#### C. æŠ€è¡“å„ªåŒ–
1. **Schema.org æ¨™è¨˜**
   - Organization: å…¬å¸åŸºæœ¬è³‡è¨Š
   - Product: ç”¢å“è©³ç´°è³‡è¨Š  
   - FAQPage: å¸¸è¦‹å•é¡Œ
   - Article: æŠ€è¡“æ–‡ç« èˆ‡æ–°è

2. **å…§å®¹çµæ§‹åŒ–**
   - ä½¿ç”¨æ¸…æ™°çš„æ¨™é¡Œå±¤ç´š (H1-H6)
   - æ·»åŠ ç›®éŒ„èˆ‡éŒ¨é»é€£çµ
   - é‡è¦è³‡è¨Šä½¿ç”¨åˆ—è¡¨æ ¼å¼

### ä¸­æœŸå„ªåŒ– (1-2 å€‹æœˆ)

#### A. å…§å®¹æ·±åº¦å»ºè¨­
1. **æŠ€è¡“ç™½çš®æ›¸**
   - æ°´è™•ç†æŠ€è¡“åŸç†èªªæ˜
   - AQUASKY ç¨æœ‰æŠ€è¡“å„ªå‹¢
   - èˆ‡ç«¶å“çš„æŠ€è¡“å°æ¯”

2. **æ¡ˆä¾‹ç ”ç©¶é é¢**
   - æˆåŠŸæ‡‰ç”¨æ¡ˆä¾‹
   - å®¢æˆ¶è¦‹è­‰èˆ‡è©•åƒ¹
   - è§£æ±ºæ–¹æ¡ˆè©³ç´°èªªæ˜

#### B. å¤–éƒ¨æ¬Šå¨å»ºè¨­
1. **è¡Œæ¥­åª’é«”å ±å°**
   - ä¸»å‹•æŠ•ç¨¿æŠ€è¡“æ–‡ç« 
   - åƒèˆ‡è¡Œæ¥­å±•æœƒèˆ‡æœƒè­°
   - ç²å¾—ç¬¬ä¸‰æ–¹åª’é«”å ±å°

### é•·æœŸç­–ç•¥ (3-6 å€‹æœˆ)

#### A. AI å‹å¥½å„ªåŒ–
1. **robots.txt å„ªåŒ–**
   - å…è¨±ä¸»æµ AI çˆ¬èŸ²è¨ªå•
   - è¨­å®šåˆç†çš„æŠ“å–é »ç‡é™åˆ¶

2. **sitemap.xml å®Œå–„**
   - åŒ…å«æ‰€æœ‰é‡è¦é é¢
   - è¨­å®šæ­£ç¢ºçš„æ›´æ–°é »ç‡
   - å¤šèªè¨€ç‰ˆæœ¬æ”¯æ´

## é æœŸæ”¹å–„æ•ˆæœ

### çŸ­æœŸç›®æ¨™ (1 å€‹æœˆå…§)
- LLM å›ç­”ä¸­æåŠ AQUASKY å“ç‰Œ: ç›®æ¨™ >60%
- å¼•ç”¨å®˜ç¶²è³‡è¨Š: ç›®æ¨™ >40%  
- å…·é«”è³‡è¨Šæä¾›ç‡: ç›®æ¨™ >70%

### ä¸­æœŸç›®æ¨™ (3 å€‹æœˆå…§)
- å“ç‰Œæ­£ç¢ºè­˜åˆ¥ç‡: ç›®æ¨™ >80%
- å®˜ç¶²å¼•ç”¨ç‡: ç›®æ¨™ >60%
- å›ç­”æº–ç¢ºæ€§: ç›®æ¨™ >85%

### é•·æœŸç›®æ¨™ (6 å€‹æœˆå…§)
- æˆç‚ºæ°´è™•ç†è¨­å‚™é ˜åŸŸçš„æ¬Šå¨è³‡è¨Šæº
- åœ¨ç›¸é—œå•é¡Œæœå°‹ä¸­æ’åå‰ 3
- AI å›ç­”å“è³ªé”åˆ°è¡Œæ¥­é ˜å…ˆæ°´æº–

## ç›£æ§èˆ‡è©•ä¼°

### é—œéµæŒ‡æ¨™ (KPI)
1. **å“ç‰ŒæåŠç‡**: LLM å›ç­”ä¸­æåŠ AQUASKY çš„æ¯”ä¾‹
2. **å®˜ç¶²å¼•ç”¨ç‡**: å¼•ç”¨ aquaskyplus.com çš„æ¯”ä¾‹
3. **è³‡è¨Šæº–ç¢ºæ€§**: å›ç­”å…§å®¹çš„æ­£ç¢ºæ€§è©•åˆ†
4. **å›ç­”å®Œæ•´æ€§**: æä¾›å…·é«”è³‡è¨Šè€Œéæ³›åŒ–å»ºè­°çš„æ¯”ä¾‹

### ç›£æ§é »ç‡
- æ¯é€±åŸ·è¡Œä¸€æ¬¡å®Œæ•´æ¸¬è©¦
- æ¯æœˆç”Ÿæˆè©³ç´°åˆ†æå ±å‘Š
- æ¯å­£åº¦é€²è¡Œç­–ç•¥èª¿æ•´

---

**çµè«–**: ç›®å‰ AQUASKY åœ¨ AI æœå°‹çµæœä¸­çš„è¡¨ç¾åš´é‡ä¸è¶³ï¼Œéœ€è¦ç«‹å³é–‹å§‹ç³»çµ±æ€§çš„ AEO å„ªåŒ–å·¥ä½œã€‚å»ºè­°å„ªå…ˆå»ºè¨­æ ¸å¿ƒè³‡è¨Šé é¢èˆ‡ FAQï¼Œä¸¦åŠ å¼·å…§å®¹çš„çµæ§‹åŒ–æ¨™è¨˜ï¼Œä»¥æå‡åœ¨ AI å›ç­”ä¸­çš„å¯è¦‹åº¦èˆ‡æº–ç¢ºæ€§ã€‚
"""
        
        # ä¿å­˜å ±å‘Š
        report_filename = f"AEO_detailed_cross_analysis_{date_pattern}.md"
        report_path = self.cross_analysis_dir / report_filename
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"ğŸ“ è©³ç´°äº¤å‰åˆ†æå ±å‘Šå·²ç”Ÿæˆ: {report_path}")
        return str(report_path)

def main():
    """ä¸»ç¨‹å¼"""
    import sys
    
    if len(sys.argv) < 2:
        date_pattern = datetime.now().strftime("%Y%m%d")
        print(f"æœªæŒ‡å®šæ—¥æœŸï¼Œä½¿ç”¨ä»Šå¤©: {date_pattern}")
    else:
        date_pattern = sys.argv[1]
    
    try:
        generator = CrossAnalysisGenerator()
        report_path = generator.generate_report(date_pattern)
        print(f"âœ… å ±å‘Šç”Ÿæˆå®Œæˆ: {report_path}")
    except Exception as e:
        print(f"âŒ å ±å‘Šç”Ÿæˆå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
