#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
äº¤å‰åˆ†æå ±å‘Šå“è³ªé©—è­‰å™¨
ç¢ºä¿æ¯æ¬¡ç”Ÿæˆçš„å ±å‘Šéƒ½ç¬¦åˆæ¨™æº–
"""

import re
from pathlib import Path
from typing import Dict, List, Tuple, Any
from analysis_config import QUALITY_CHECKS, QUALITY_STANDARDS, KPI_TARGETS

class ReportQualityValidator:
    """å ±å‘Šå“è³ªé©—è­‰å™¨"""
    
    def __init__(self):
        self.checks = QUALITY_CHECKS
        self.standards = QUALITY_STANDARDS
        self.targets = KPI_TARGETS
    
    def validate_report(self, report_path: str) -> Dict[str, Any]:
        """é©—è­‰å ±å‘Šå“è³ª"""
        report_path = Path(report_path)
        
        if not report_path.exists():
            return {
                'valid': False,
                'error': f'å ±å‘Šæª”æ¡ˆä¸å­˜åœ¨: {report_path}'
            }
        
        try:
            with open(report_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except Exception as e:
            return {
                'valid': False,
                'error': f'ç„¡æ³•è®€å–å ±å‘Šæª”æ¡ˆ: {e}'
            }
        
        # åŸ·è¡Œå„é …æª¢æŸ¥
        results = {
            'valid': True,
            'checks': {},
            'warnings': [],
            'errors': []
        }
        
        # 1. å…§å®¹é•·åº¦æª¢æŸ¥
        content_length = len(content)
        results['checks']['content_length'] = {
            'actual': content_length,
            'required': self.checks['min_content_length'],
            'passed': content_length >= self.checks['min_content_length']
        }
        
        if not results['checks']['content_length']['passed']:
            results['errors'].append(f"å ±å‘Šå…§å®¹éçŸ­: {content_length} < {self.checks['min_content_length']}")
        
        # 2. å¿…éœ€ç« ç¯€æª¢æŸ¥
        required_sections = [
            'åŸ·è¡Œæ‘˜è¦', 'æ¨¡å‹æ•ˆèƒ½æ¯”è¼ƒ', 'é—œéµå•é¡Œåˆ†æ', 'æ¨¡å‹è¡¨ç¾è©•ç´š',
            'æ ¹æœ¬å•é¡Œè¨ºæ–·', 'ç¶²ç«™å…§å®¹æ”¹å–„å»ºè­°', 'é æœŸæ”¹å–„æ•ˆæœ', 'ç›£æ§èˆ‡è©•ä¼°'
        ]
        
        found_sections = []
        for section in required_sections:
            if section in content:
                found_sections.append(section)
        
        results['checks']['required_sections'] = {
            'found': len(found_sections),
            'required': len(required_sections),
            'passed': len(found_sections) >= len(required_sections)
        }
        
        if not results['checks']['required_sections']['passed']:
            missing = set(required_sections) - set(found_sections)
            results['errors'].append(f"ç¼ºå°‘å¿…éœ€ç« ç¯€: {', '.join(missing)}")
        
        # 3. æ•¸æ“šæŒ‡æ¨™æª¢æŸ¥
        metrics_pattern = r'(\d+\.?\d*)%'
        metrics_found = re.findall(metrics_pattern, content)
        
        results['checks']['metrics_count'] = {
            'found': len(metrics_found),
            'required': 10,  # è‡³å°‘æ‡‰æœ‰10å€‹ç™¾åˆ†æ¯”æ•¸æ“š
            'passed': len(metrics_found) >= 10
        }
        
        if not results['checks']['metrics_count']['passed']:
            results['warnings'].append(f"æ•¸æ“šæŒ‡æ¨™è¼ƒå°‘: æ‰¾åˆ° {len(metrics_found)} å€‹ï¼Œå»ºè­°è‡³å°‘ 10 å€‹")
        
        # 4. æ¨¡å‹æ’åæª¢æŸ¥
        ranking_patterns = [
            r'Token ä½¿ç”¨æ•ˆç‡æ’å',
            r'å›ç­”å“è³ªæ’å',
            r'æœ€ä½³ç¶œåˆè¡¨ç¾',
            r'æœ€ä½³æ•ˆç‡è¡¨ç¾',
            r'æœ€ä½³æ€§åƒ¹æ¯”'
        ]
        
        found_rankings = []
        for pattern in ranking_patterns:
            if re.search(pattern, content):
                found_rankings.append(pattern)
        
        results['checks']['rankings'] = {
            'found': len(found_rankings),
            'required': len(ranking_patterns),
            'passed': len(found_rankings) >= 3
        }
        
        if not results['checks']['rankings']['passed']:
            results['warnings'].append(f"æ’åé¡å‹ä¸è¶³: æ‰¾åˆ° {len(found_rankings)} å€‹")
        
        # 5. æ”¹å–„å»ºè­°æª¢æŸ¥
        suggestion_patterns = [
            r'ç«‹å³åŸ·è¡Œ',
            r'ä¸­æœŸå„ªåŒ–',
            r'é•·æœŸç­–ç•¥',
            r'å»ºç«‹.*é é¢',
            r'Schema.*æ¨™è¨˜',
            r'FAQ.*é é¢'
        ]
        
        found_suggestions = []
        for pattern in suggestion_patterns:
            matches = re.findall(pattern, content)
            found_suggestions.extend(matches)
        
        results['checks']['suggestions'] = {
            'found': len(found_suggestions),
            'required': self.checks['min_suggestions'],
            'passed': len(found_suggestions) >= self.checks['min_suggestions']
        }
        
        if not results['checks']['suggestions']['passed']:
            results['warnings'].append(f"æ”¹å–„å»ºè­°ä¸è¶³: æ‰¾åˆ° {len(found_suggestions)} å€‹")
        
        # 6. KPIç›®æ¨™æª¢æŸ¥
        kpi_patterns = [
            r'çŸ­æœŸç›®æ¨™',
            r'ä¸­æœŸç›®æ¨™',
            r'é•·æœŸç›®æ¨™',
            r'å“ç‰Œ.*ç‡.*>.*%',
            r'å®˜ç¶².*ç‡.*>.*%'
        ]
        
        found_kpis = []
        for pattern in kpi_patterns:
            if re.search(pattern, content):
                found_kpis.append(pattern)
        
        results['checks']['kpi_targets'] = {
            'found': len(found_kpis),
            'required': 3,
            'passed': len(found_kpis) >= 3
        }
        
        if not results['checks']['kpi_targets']['passed']:
            results['warnings'].append("KPIç›®æ¨™è¨­å®šä¸å®Œæ•´")
        
        # 7. çµè«–æª¢æŸ¥
        has_conclusion = 'çµè«–' in content and len(content.split('çµè«–')[-1]) > 100
        results['checks']['conclusion'] = {
            'found': has_conclusion,
            'required': True,
            'passed': has_conclusion
        }
        
        if not results['checks']['conclusion']['passed']:
            results['errors'].append("ç¼ºå°‘å®Œæ•´çµè«–")
        
        # ç¸½é«”é©—è­‰çµæœ
        all_critical_passed = all(
            check['passed'] for check in results['checks'].values()
            if check.get('required') in [True, results['checks'][list(results['checks'].keys())[0]]['required']]
        )
        
        results['valid'] = len(results['errors']) == 0
        results['quality_score'] = self._calculate_quality_score(results['checks'])
        
        return results
    
    def _calculate_quality_score(self, checks: Dict[str, Any]) -> float:
        """è¨ˆç®—å“è³ªåˆ†æ•¸ (0-100)"""
        total_score = 0
        max_score = 0
        
        weights = {
            'content_length': 20,
            'required_sections': 25,
            'metrics_count': 15,
            'rankings': 15,
            'suggestions': 15,
            'kpi_targets': 5,
            'conclusion': 5
        }
        
        for check_name, check_result in checks.items():
            weight = weights.get(check_name, 10)
            max_score += weight
            
            if check_result['passed']:
                total_score += weight
            else:
                # éƒ¨åˆ†åˆ†æ•¸åŸºæ–¼å®Œæˆåº¦
                if 'found' in check_result and 'required' in check_result:
                    completion_rate = min(check_result['found'] / check_result['required'], 1.0)
                    total_score += weight * completion_rate
        
        return (total_score / max_score) * 100 if max_score > 0 else 0
    
    def generate_quality_report(self, validation_result: Dict[str, Any]) -> str:
        """ç”Ÿæˆå“è³ªæª¢æŸ¥å ±å‘Š"""
        result = validation_result
        
        report = f"""# äº¤å‰åˆ†æå ±å‘Šå“è³ªæª¢æŸ¥çµæœ

## ç¸½é«”è©•ä¼°
- **é©—è­‰ç‹€æ…‹**: {'âœ… é€šé' if result['valid'] else 'âŒ æœªé€šé'}
- **å“è³ªåˆ†æ•¸**: {result['quality_score']:.1f}/100

## è©³ç´°æª¢æŸ¥çµæœ

"""
        
        for check_name, check_result in result['checks'].items():
            status = 'âœ…' if check_result['passed'] else 'âŒ'
            report += f"### {check_name.replace('_', ' ').title()}\n"
            report += f"{status} **ç‹€æ…‹**: {'é€šé' if check_result['passed'] else 'æœªé€šé'}\n"
            
            if 'found' in check_result and 'required' in check_result:
                report += f"- ç™¼ç¾: {check_result['found']}\n"
                report += f"- è¦æ±‚: {check_result['required']}\n"
            elif 'actual' in check_result and 'required' in check_result:
                report += f"- å¯¦éš›: {check_result['actual']}\n"
                report += f"- è¦æ±‚: {check_result['required']}\n"
            
            report += "\n"
        
        if result['errors']:
            report += "## âŒ éŒ¯èª¤é …ç›®\n"
            for error in result['errors']:
                report += f"- {error}\n"
            report += "\n"
        
        if result['warnings']:
            report += "## âš ï¸ è­¦å‘Šé …ç›®\n"
            for warning in result['warnings']:
                report += f"- {warning}\n"
            report += "\n"
        
        # æ”¹å–„å»ºè­°
        if not result['valid'] or result['quality_score'] < 80:
            report += "## ğŸ”§ æ”¹å–„å»ºè­°\n"
            
            if result['quality_score'] < 60:
                report += "- å ±å‘Šå“è³ªåš´é‡ä¸è¶³ï¼Œå»ºè­°é‡æ–°ç”Ÿæˆ\n"
            elif result['quality_score'] < 80:
                report += "- å ±å‘Šå“è³ªæœ‰å¾…æ”¹å–„ï¼Œå»ºè­°è£œå……ç¼ºå¤±å…§å®¹\n"
            
            for check_name, check_result in result['checks'].items():
                if not check_result['passed']:
                    if check_name == 'content_length':
                        report += "- å¢åŠ å ±å‘Šå…§å®¹æ·±åº¦å’Œè©³ç´°åˆ†æ\n"
                    elif check_name == 'required_sections':
                        report += "- è£œå……ç¼ºå¤±çš„å ±å‘Šç« ç¯€\n"
                    elif check_name == 'metrics_count':
                        report += "- æ·»åŠ æ›´å¤šé‡åŒ–æ•¸æ“šå’ŒæŒ‡æ¨™\n"
                    elif check_name == 'rankings':
                        report += "- å®Œå–„æ¨¡å‹æ’åå’Œæ¯”è¼ƒåˆ†æ\n"
                    elif check_name == 'suggestions':
                        report += "- å¢åŠ å…·é«”çš„æ”¹å–„å»ºè­°\n"
        
        return report

def main():
    """ä¸»ç¨‹å¼ - é©—è­‰æœ€æ–°çš„å ±å‘Š"""
    import sys
    from datetime import datetime
    
    if len(sys.argv) < 2:
        # å°‹æ‰¾æœ€æ–°çš„å ±å‘Šæª”æ¡ˆ
        cross_analysis_dir = Path("outputs/cross_analysis")
        if cross_analysis_dir.exists():
            reports = list(cross_analysis_dir.glob("AEO_detailed_cross_analysis_*.md"))
            if reports:
                latest_report = max(reports, key=lambda x: x.stat().st_mtime)
                report_path = str(latest_report)
            else:
                print("âŒ æ‰¾ä¸åˆ°äº¤å‰åˆ†æå ±å‘Šæª”æ¡ˆ")
                return
        else:
            print("âŒ æ‰¾ä¸åˆ° outputs/cross_analysis ç›®éŒ„")
            return
    else:
        report_path = sys.argv[1]
    
    print(f"ğŸ” é©—è­‰å ±å‘Š: {report_path}")
    
    validator = ReportQualityValidator()
    result = validator.validate_report(report_path)
    
    # ç”Ÿæˆå“è³ªæª¢æŸ¥å ±å‘Š
    quality_report = validator.generate_quality_report(result)
    
    # ä¿å­˜å“è³ªæª¢æŸ¥å ±å‘Š
    report_name = Path(report_path).stem
    quality_report_path = Path("outputs/cross_analysis") / f"{report_name}_quality_check.md"
    
    with open(quality_report_path, 'w', encoding='utf-8') as f:
        f.write(quality_report)
    
    print(f"ğŸ“Š å“è³ªåˆ†æ•¸: {result['quality_score']:.1f}/100")
    print(f"ğŸ“ å“è³ªæª¢æŸ¥å ±å‘Š: {quality_report_path}")
    
    if result['valid']:
        print("âœ… å ±å‘Šå“è³ªé©—è­‰é€šé")
    else:
        print("âŒ å ±å‘Šå“è³ªé©—è­‰æœªé€šé")
        for error in result['errors']:
            print(f"   - {error}")

if __name__ == "__main__":
    main()
